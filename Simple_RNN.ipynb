{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJWF_V8fO9jV"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "T = 8  # Sequence length\n",
        "D = 2  # Input dimensionality\n",
        "M = 3  # Hidden layer size\n",
        "\n",
        "X = np.random.randn(1, T, D)  # 1 sample of size T x D. We can think it as a single sentence of word vectors or some other vector signal"
      ],
      "metadata": {
        "id": "uu_Gfw-CPQKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCnCmJEQR5fg",
        "outputId": "36ec9c0e-5a0f-4857-f69c-6be2e7356a6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[-0.86759788,  0.44817691],\n",
              "        [-1.57981326, -0.12402025],\n",
              "        [-0.49810227,  0.07832132],\n",
              "        [-0.74875906,  0.87991938],\n",
              "        [-1.03513476, -1.3308771 ],\n",
              "        [-0.27781837, -0.58284598],\n",
              "        [-1.36489798,  1.27821434],\n",
              "        [-0.79758997,  0.8383544 ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm1():\n",
        "  input_ = Input(shape=(T, D))\n",
        "  rnn = LSTM(M, return_state=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs=input_, outputs=x)\n",
        "  o, h, c = model.predict(X)\n",
        "  print(\"o:\", o)\n",
        "  print(\"h:\", h)\n",
        "  print(\"c:\", c)\n",
        "\n"
      ],
      "metadata": {
        "id": "wPkKzJ3JSDVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm2():\n",
        "  input_ = Input(shape=(T, D))\n",
        "  rnn = LSTM(M, return_state=True, return_sequences=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs=input_, outputs=x)\n",
        "  o, h, c = model.predict(X)\n",
        "  print(\"o: \", o)\n",
        "  print(\"h: \", h)\n",
        "  print(\"c: \", c)"
      ],
      "metadata": {
        "id": "WV0Z1Dt2ZJ8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gru1():\n",
        "  input_ = Input(shape=(T, D))\n",
        "  rnn = GRU(M, return_state=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs=input_, outputs=x)\n",
        "  o, h = model.predict(X)\n",
        "  print(\"o: \", o)\n",
        "  print(\"h: \", h)"
      ],
      "metadata": {
        "id": "NdTEwZzGZTce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gru2():\n",
        "  input_ = Input(shape=(T, D))\n",
        "  rnn = GRU(M, return_state=True, return_sequences=True)\n",
        "  x = rnn(input_)\n",
        "\n",
        "  model = Model(inputs=input_, outputs=x)\n",
        "  o, h = model.predict(X)\n",
        "\n",
        "  print(\"o: \", o)\n",
        "  print(\"h: \", h)"
      ],
      "metadata": {
        "id": "2hvZ7qw1LJRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"lstm1:\")\n",
        "lstm1()\n",
        "print(\"lstm2:\")\n",
        "lstm2()\n",
        "print(\"gru1:\")\n",
        "gru1()\n",
        "print(\"gru2:\")\n",
        "gru2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYsb0yCWMQCy",
        "outputId": "aee0af09-e805-4b53-9595-b0664a4c98b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f78bc48ba30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 417ms/step\n",
            "o: [[-0.19989757 -0.27115613 -0.40757406]]\n",
            "h: [[-0.19989757 -0.27115613 -0.40757406]]\n",
            "c: [[-0.43639052 -0.6221388  -0.96941066]]\n",
            "lstm2:\n",
            "1/1 [==============================] - 1s 694ms/step\n",
            "o:  [[[-0.08655925  0.11818284 -0.04291387]\n",
            "  [-0.0773178   0.29644388 -0.2877244 ]\n",
            "  [-0.11155789  0.2316363  -0.22630101]\n",
            "  [-0.27797928  0.24164784 -0.15470032]\n",
            "  [ 0.0048619   0.228338   -0.30628893]\n",
            "  [ 0.06626178  0.06347234 -0.29895177]\n",
            "  [-0.16867034  0.1712563  -0.30675367]\n",
            "  [-0.30545485  0.22650757 -0.16295226]]]\n",
            "h:  [[-0.30545485  0.22650757 -0.16295226]]\n",
            "c:  [[-0.54507697  0.5418178  -0.25880367]]\n",
            "gru1:\n",
            "1/1 [==============================] - 1s 603ms/step\n",
            "o:  [[ 0.45756215  0.2549777  -0.42668056]]\n",
            "h:  [[ 0.45756215  0.2549777  -0.42668056]]\n",
            "gru2:\n",
            "1/1 [==============================] - 1s 597ms/step\n",
            "o:  [[[-0.19373484  0.03487708 -0.21713026]\n",
            "  [-0.3942749   0.1002564  -0.3936553 ]\n",
            "  [-0.31831706  0.07262447 -0.39783007]\n",
            "  [-0.3261702   0.07547873 -0.4934755 ]\n",
            "  [-0.458644    0.10551018 -0.24729882]\n",
            "  [-0.27293393  0.04431827 -0.10385965]\n",
            "  [-0.36503989  0.0738878  -0.27536282]\n",
            "  [-0.36297157  0.07389672 -0.4068069 ]]]\n",
            "h:  [[-0.36297157  0.07389672 -0.4068069 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3uhM-WmMhSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}